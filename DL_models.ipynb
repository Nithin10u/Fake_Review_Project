{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nithin Srinivaas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 31ms/step - accuracy: 0.6708 - loss: 0.6042 - precision: 0.6749 - recall: 0.6552 - val_accuracy: 0.7364 - val_loss: 0.5315 - val_precision: 0.8005 - val_recall: 0.6326\n",
      "Epoch 2/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.7485 - loss: 0.5327 - precision: 0.7403 - recall: 0.7631 - val_accuracy: 0.7806 - val_loss: 0.4932 - val_precision: 0.7660 - val_recall: 0.8106\n",
      "Epoch 3/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 31ms/step - accuracy: 0.7588 - loss: 0.5169 - precision: 0.7663 - recall: 0.7425 - val_accuracy: 0.5233 - val_loss: 0.7166 - val_precision: 0.5191 - val_recall: 0.6828\n",
      "Epoch 4/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.5943 - loss: 0.6639 - precision: 0.5783 - recall: 0.6870 - val_accuracy: 0.5560 - val_loss: 0.7231 - val_precision: 0.5600 - val_recall: 0.5387\n",
      "Epoch 5/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 31ms/step - accuracy: 0.6615 - loss: 0.6240 - precision: 0.6483 - recall: 0.7011 - val_accuracy: 0.6313 - val_loss: 0.6465 - val_precision: 0.6243 - val_recall: 0.6668\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
      "Classification Report for RNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.80      0.76      0.78      4016\n",
      "          OR       0.77      0.81      0.79      4071\n",
      "\n",
      "    accuracy                           0.78      8087\n",
      "   macro avg       0.78      0.78      0.78      8087\n",
      "weighted avg       0.78      0.78      0.78      8087\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nithin Srinivaas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 72ms/step - accuracy: 0.5576 - loss: 0.6684 - precision_1: 0.5911 - recall_1: 0.3651 - val_accuracy: 0.6690 - val_loss: 0.6260 - val_precision_1: 0.7189 - val_recall_1: 0.5593\n",
      "Epoch 2/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 60ms/step - accuracy: 0.7683 - loss: 0.4872 - precision_1: 0.7933 - recall_1: 0.7238 - val_accuracy: 0.8766 - val_loss: 0.2881 - val_precision_1: 0.8995 - val_recall_1: 0.8491\n",
      "Epoch 3/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - accuracy: 0.9009 - loss: 0.2377 - precision_1: 0.9044 - recall_1: 0.8960 - val_accuracy: 0.9034 - val_loss: 0.2264 - val_precision_1: 0.9107 - val_recall_1: 0.8953\n",
      "Epoch 4/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 52ms/step - accuracy: 0.9316 - loss: 0.1695 - precision_1: 0.9349 - recall_1: 0.9274 - val_accuracy: 0.9017 - val_loss: 0.2304 - val_precision_1: 0.9310 - val_recall_1: 0.8685\n",
      "Epoch 5/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.9510 - loss: 0.1252 - precision_1: 0.9544 - recall_1: 0.9470 - val_accuracy: 0.9042 - val_loss: 0.2582 - val_precision_1: 0.8846 - val_recall_1: 0.9304\n",
      "Epoch 6/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 56ms/step - accuracy: 0.9627 - loss: 0.0970 - precision_1: 0.9668 - recall_1: 0.9581 - val_accuracy: 0.9021 - val_loss: 0.2622 - val_precision_1: 0.8862 - val_recall_1: 0.9236\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step\n",
      "Classification Report for LSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.90      0.90      0.90      4016\n",
      "          OR       0.90      0.90      0.90      4071\n",
      "\n",
      "    accuracy                           0.90      8087\n",
      "   macro avg       0.90      0.90      0.90      8087\n",
      "weighted avg       0.90      0.90      0.90      8087\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nithin Srinivaas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 78ms/step - accuracy: 0.8453 - loss: 0.3305 - precision_2: 0.8659 - recall_2: 0.8160 - val_accuracy: 0.8941 - val_loss: 0.2366 - val_precision_2: 0.9360 - val_recall_2: 0.8469\n",
      "Epoch 2/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 75ms/step - accuracy: 0.9241 - loss: 0.1864 - precision_2: 0.9229 - recall_2: 0.9250 - val_accuracy: 0.9141 - val_loss: 0.2100 - val_precision_2: 0.8970 - val_recall_2: 0.9362\n",
      "Epoch 3/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 76ms/step - accuracy: 0.9461 - loss: 0.1323 - precision_2: 0.9458 - recall_2: 0.9461 - val_accuracy: 0.9127 - val_loss: 0.2108 - val_precision_2: 0.9244 - val_recall_2: 0.8996\n",
      "Epoch 4/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 75ms/step - accuracy: 0.9610 - loss: 0.0985 - precision_2: 0.9601 - recall_2: 0.9616 - val_accuracy: 0.9110 - val_loss: 0.2547 - val_precision_2: 0.9141 - val_recall_2: 0.9079\n",
      "Epoch 5/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 76ms/step - accuracy: 0.9682 - loss: 0.0811 - precision_2: 0.9686 - recall_2: 0.9675 - val_accuracy: 0.9102 - val_loss: 0.2562 - val_precision_2: 0.9270 - val_recall_2: 0.8913\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step\n",
      "Classification Report for BiLSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.93      0.89      0.91      4016\n",
      "          OR       0.89      0.93      0.91      4071\n",
      "\n",
      "    accuracy                           0.91      8087\n",
      "   macro avg       0.91      0.91      0.91      8087\n",
      "weighted avg       0.91      0.91      0.91      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, SimpleRNN\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('TP_DS.csv')  # Replace with your file path\n",
    "data['label_encoded'] = LabelEncoder().fit_transform(data['label'])\n",
    "\n",
    "# Split data\n",
    "X = data['cleaned_text']\n",
    "y = data['label_encoded']\n",
    "X = data['cleaned_text'].astype(str)  # Convert to string to handle any float or NaN issues\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization and Padding\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Define a function to build models\n",
    "def build_model(model_type=\"RNN\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(64, return_sequences=False))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(64, return_sequences=False))\n",
    "    elif model_type == \"BiLSTM\":\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "        \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_and_evaluate(model_type):\n",
    "    model = build_model(model_type)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train_pad, y_train, \n",
    "                        epochs=10, \n",
    "                        batch_size=64, \n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "    print(f\"Classification Report for {model_type}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['CG', 'OR']))\n",
    "    \n",
    "# Train and evaluate RNN, LSTM, and BiLSTM models\n",
    "for model_type in [\"RNN\", \"LSTM\", \"BiLSTM\"]:\n",
    "    train_and_evaluate(model_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers and PreTrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.3.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nithin srinivaas\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\Users\\Nithin Srinivaas\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n",
      "Python version: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "Transformers version: 4.57.1\n",
      "Transformers file: c:\\Users\\Nithin Srinivaas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\__init__.py\n",
      "TrainingArguments __init__ signature: (self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: float = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: bool = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: bool = True, label_names: Optional[list[str]] = None, load_best_model_at_end: bool = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = None, fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, parallelism_config: Optional[accelerate.parallelism_config.ParallelismConfig] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: str = 'length', report_to: Union[NoneType, str, list[str]] = None, project: str = 'huggingface', trackio_space_id: Optional[str] = 'trackio', ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: bool = False, include_num_input_tokens_seen: Union[str, bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: bool = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: bool = False, average_tokens_across_devices: bool = True) -> None\n",
      "TrainingArguments defined in: c:\\Users\\Nithin Srinivaas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\training_args.py\n"
     ]
    }
   ],
   "source": [
    "import sys, inspect\n",
    "import transformers\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Python version:\", sys.version.splitlines()[0])\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "print(\"Transformers file:\", transformers.__file__)\n",
    "print(\"TrainingArguments __init__ signature:\", inspect.signature(TrainingArguments.__init__))\n",
    "print(\"TrainingArguments defined in:\", inspect.getsourcefile(TrainingArguments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Nithin Srinivaas\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Nithin Srinivaas\\AppData\\Local\\Temp\\ipykernel_16904\\4286014322.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12132' max='12132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12132/12132 1:32:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.375257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.274200</td>\n",
       "      <td>0.316264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.204600</td>\n",
       "      <td>0.287347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.90      0.94      0.92      4016\n",
      "          OR       0.94      0.90      0.92      4071\n",
      "\n",
      "    accuracy                           0.92      8087\n",
      "   macro avg       0.92      0.92      0.92      8087\n",
      "weighted avg       0.92      0.92      0.92      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used (CPU or GPU)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset and preprocess\n",
    "data = pd.read_csv('TP_DS.csv')  # Replace with your file path\n",
    "data['label_encoded'] = LabelEncoder().fit_transform(data['label'])\n",
    "\n",
    "# Split dataset\n",
    "X = data['cleaned_text'].astype(str)\n",
    "y = data['label_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset class to handle text and labels\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten().to(device),\n",
    "            'attention_mask': encoding['attention_mask'].flatten().to(device),\n",
    "            'labels': torch.tensor(label, dtype=torch.long).to(device)\n",
    "        }\n",
    "\n",
    "# Create dataset and data loaders\n",
    "train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = ReviewDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define Trainer for training BERT model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = torch.argmax(torch.tensor(predictions.predictions), axis=1).numpy()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_labels, target_names=['CG', 'OR']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and training arguments saved in BERT./saved_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define a directory to save the model and tokenizer\n",
    "model_dir = 'BERT./saved_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained BERT model\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "# Save training arguments as a JSON file\n",
    "with open(os.path.join(model_dir, 'training_args.json'), 'w') as f:\n",
    "    json.dump(training_args.to_dict(), f)\n",
    "\n",
    "print(f\"Model, tokenizer, and training arguments saved in {model_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "Number of GPUs available: 1\n",
      "GPU Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This saved setup can be used directly for deployment, where you can reload the model and tokenizer using the from_pretrained method as follows:'''\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the model and tokenizer for deployment\n",
    "loaded_model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(model_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
